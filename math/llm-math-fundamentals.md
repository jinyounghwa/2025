# LLM을 위한 기초 수학 - 완전 가이드

**난이도:** 기초  
**대상:** 웹/앱 개발자 (비수학 전공)  
**목표:** LLM 원리 이해와 실무 적용

---

## 📖 목차

1. [벡터와 임베딩](#1-벡터와-임베딩)
2. [닷 프로덕트](#2-닷-프로덕트)
3. [벡터 정규화](#3-벡터-정규화)
4. [코사인 유사도](#4-코사인-유사도)
5. [소프트맥스](#5-소프트맥스)
6. [체인 룰과 미분](#6-체인-룰과-미분)
7. [조건부 확률](#7-조건부-확률)
8. [함께 이해하기: 실제 LLM 흐름](#8-함께-이해하기-실제-llm-흐름)

---

## 1. 벡터와 임베딩

### 핵심 개념

LLM의 첫 번째 단계는 **텍스트를 숫자 배열(벡터)로 변환**하는 것입니다. 컴퓨터는 글자를 직접 계산할 수 없기 때문입니다.

### 예시: 한국어 텍스트 변환

```
입력 텍스트: "안녕하세요"

↓ (임베딩 모델을 통과)

출력 벡터: [0.2, -0.5, 0.8, 0.1, 0.3, -0.2, ... ] (768차원)
```

각 숫자는 **의미(semantic meaning)**의 다른 측면을 나타냅니다. 첫 번째 숫자는 "긍정성", 두 번째는 "높은 에너지", 세 번째는 "인사 관련성" 같은 추상적 개념을 담고 있습니다.

### 왜 중요한가?

**아날로지**: 당신의 앱에서 사용자 프로필을 저장할 때:

```
사용자 = {
  이름: "김철수",
  나이: 28,
  관심사: ["개발", "게임"],
  지역: "서울"
}
```

이 정보를 **구조화된 데이터**로 저장하는 것처럼, LLM도 의미를 **구조화된 숫자**로 저장합니다.

### 개발 실무: RAG 시스템

당신이 RAG(Retrieval-Augmented Generation) 검색을 만들 때:

```python
# 1단계: 모든 문서를 벡터로 변환 (미리 해둠)
document_1 = "오늘 날씨가 좋습니다"
vector_1 = embedding_model.encode(document_1)  # [0.1, 0.2, ..., 0.8]

document_2 = "내일은 비가 올 것 같습니다"
vector_2 = embedding_model.encode(document_2)  # [-0.3, 0.1, ..., 0.5]

# 2단계: 사용자 질문도 벡터로 변환
query = "날씨는 어떻게 되나요?"
query_vector = embedding_model.encode(query)    # [0.15, 0.18, ..., 0.7]

# 3단계: 가장 유사한 문서 찾기 (다음 배울 내용)
```

### 기억할 점

- 벡터 = 텍스트의 **수치적 표현**
- 각 차원은 추상적 의미를 담음
- 같은 의미의 텍스트는 **비슷한 벡터**를 생성함

---

## 2. 닷 프로덕트

### 핵심 개념

두 벡터 간의 **관계를 측정**하는 기본 연산입니다.

### 수식

```
A · B = (a₁ × b₁) + (a₂ × b₂) + (a₃ × b₃) + ...
```

### 단계별 예시

```
A = [1, 2, 3]
B = [4, 5, 6]

단계 1: 첫 번째 원소 곱하기
  1 × 4 = 4

단계 2: 두 번째 원소 곱하기
  2 × 5 = 10

단계 3: 세 번째 원소 곱하기
  3 × 6 = 18

단계 4: 모두 더하기
  4 + 10 + 18 = 32

A · B = 32
```

### 결과의 의미

- **큰 양수** (예: 32) → 두 벡터가 **같은 방향**
- **0에 가까움** → 두 벡터가 **무관**
- **음수** → 두 벡터가 **반대 방향**

### 직관적 이해: 방향

```
→ 와 → 를 비교: 닷 프로덕트 = 큼 (같은 방향)
→ 와 ← 를 비교: 닷 프로덕트 = 작음 (반대 방향)
→ 와 ↑ 를 비교: 닷 프로덕트 = 0에 가까움 (수직)
```

### LLM에서의 역할

Transformer 모델의 **어텐션(Attention)** 메커니즘이 매번 닷 프로덕트를 계산합니다:

```
단어 1: "오늘"    벡터: [0.1, 0.2, 0.3]
단어 2: "날씨"    벡터: [0.15, 0.25, 0.35]

닷 프로덕트 = 0.1×0.15 + 0.2×0.25 + 0.3×0.35
            = 0.015 + 0.05 + 0.105
            = 0.17

→ 큰 값이면 두 단어가 관련이 있다는 신호
```

### 코드 예시 (Python)

```python
import numpy as np

A = np.array([1, 2, 3])
B = np.array([4, 5, 6])

dot_product = np.dot(A, B)
print(dot_product)  # 32
```

### 기억할 점

- 닷 프로덕트 = 두 벡터의 **관계도**
- 방향이 같을수록 결과가 커짐
- LLM에서 **어텐션 계산**의 기초

---

## 3. 벡터 정규화

### 문제 상황

닷 프로덕트는 벡터의 **크기(magnitude)**에 민감합니다:

```
작은 벡터: A = [1, 1]
큰 벡터:   B = [10, 10]

이 두 벡터는 방향이 완전히 같습니다.
(B는 A에 10을 곱한 것)

하지만 닷 프로덕트 결과:
A · A = 1×1 + 1×1 = 2
B · B = 10×10 + 10×10 = 200

같은 방향인데 결과가 다릅니다!
```

### 해결책: 벡터 정규화

벡터의 길이를 정확히 **1**로 만듭니다.

### 수식

```
정규화된 벡터 = 원본 벡터 / 벡터의 길이

벡터의 길이(크기) = √(a₁² + a₂² + a₃² + ...)
```

### 단계별 예시

```
원본 벡터: A = [3, 4]

단계 1: 각 원소를 제곱
  3² = 9
  4² = 16

단계 2: 모두 더하기
  9 + 16 = 25

단계 3: 제곱근 구하기
  √25 = 5
  (이것이 벡터의 길이)

단계 4: 각 원소를 길이로 나누기
  정규화된 A = [3/5, 4/5] = [0.6, 0.8]

단계 5: 검증 (길이가 1인지 확인)
  0.6² + 0.8² = 0.36 + 0.64 = 1.0 ✓
```

### 정규화된 벡터의 특징

```
원본:     A = [3, 4]      (길이 = 5)
정규화:   Â = [0.6, 0.8]  (길이 = 1)

방향은 정확히 같음! ✓
크기만 다름.
```

### LLM 실무에서의 의미

대부분의 LLM 임베딩 모델은 **이미 정규화된 벡터**를 반환합니다:

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

text = "좋은 아침입니다"
embedding = model.encode(text, normalize_embeddings=True)

# embedding은 이미 정규화됨 (길이 = 1)
```

### 코드 예시

```python
import numpy as np

# 원본 벡터
A = np.array([3, 4])

# 길이 계산
magnitude = np.linalg.norm(A)
print(f"길이: {magnitude}")  # 5.0

# 정규화
normalized_A = A / magnitude
print(f"정규화된 벡터: {normalized_A}")  # [0.6 0.8]

# 검증
print(f"정규화된 벡터의 길이: {np.linalg.norm(normalized_A)}")  # 1.0
```

### 기억할 점

- 정규화 = 방향은 유지, 길이는 1로 만들기
- RAG 시스템에서 공정한 비교를 위해 필수
- 많은 모델이 자동으로 정규화함

---

## 4. 코사인 유사도

### 핵심 개념

두 벡터 사이의 **각도**를 측정합니다. 크기는 무시하고 **방향만** 비교합니다.

### 수식

```
cos(θ) = (A · B) / (|A| × |B|)

여기서:
- A · B = 닷 프로덕트
- |A| = 벡터 A의 길이
- |B| = 벡터 B의 길이
- θ = 두 벡터 사이의 각도
```

### 단계별 예시

```
A = [1, 0]     (오른쪽 방향)
B = [1, 1]     (우상향 방향)

단계 1: 닷 프로덕트 계산
  A · B = 1×1 + 0×1 = 1

단계 2: 각 벡터의 길이 계산
  |A| = √(1² + 0²) = 1
  |B| = √(1² + 1²) = √2 ≈ 1.414

단계 3: 나누기
  cos(θ) = 1 / (1 × 1.414) ≈ 0.707
```

### 해석: 결과의 의미

```
코사인 유사도 범위: -1 ~ 1

1.0에 가까움 (예: 0.95)
↓
두 벡터가 같은 방향 (매우 유사한 의미)
예: "좋아요" vs "훌륭합니다"

0.5 정도
↓
어느 정도 관련 있음
예: "날씨" vs "해"

0에 가까움
↓
관련 없음
예: "숫자 2" vs "행복"

-1.0에 가까움 (예: -0.95)
↓
반대 의미
예: "좋아요" vs "싫어요"
```

### 시각화

```
     B (우상향)
     ↗ /
    / /
   / / 각도 ≈ 45도
  / /
 A → (오른쪽)

cos(45도) ≈ 0.707
```

### LLM 실무: RAG 검색 시스템

```python
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# 문서들의 임베딩 (미리 계산됨)
doc1_vector = np.array([0.2, -0.5, 0.8, 0.1])
doc2_vector = np.array([0.19, -0.52, 0.75, 0.15])
doc3_vector = np.array([-0.8, 0.1, -0.2, 0.9])

# 사용자 질문의 임베딩
query_vector = np.array([0.21, -0.48, 0.82, 0.12])

# 코사인 유사도 계산
sim1 = cosine_similarity([query_vector], [doc1_vector])[0][0]
sim2 = cosine_similarity([query_vector], [doc2_vector])[0][0]
sim3 = cosine_similarity([query_vector], [doc3_vector])[0][0]

print(f"쿼리와 문서1의 유사도: {sim1:.3f}")  # 0.997 (매우 유사)
print(f"쿼리와 문서2의 유사도: {sim2:.3f}")  # 0.995 (매우 유사)
print(f"쿼리와 문서3의 유사도: {sim3:.3f}")  # 0.125 (관련 없음)

# 실제 검색 로직
threshold = 0.8
results = []
if sim1 > threshold:
    results.append("문서1")
if sim2 > threshold:
    results.append("문서2")
if sim3 > threshold:
    results.append("문서3")

print(f"검색 결과: {results}")  # ["문서1", "문서2"]
```

### 코사인 유사도의 장점

1. **크기 무시** - 벡터 길이는 상관없음
2. **0~1 범위** - 정규화된 벡터면 더 간단
3. **직관적** - 각도를 직접 나타냄
4. **계산 효율** - 매우 빠름

### 기억할 점

- 코사인 유사도 = **방향 기반** 유사도
- RAG, 검색, 추천 시스템의 핵심
- -1 ~ 1 범위, 1에 가까울수록 유사

---

## 5. 소프트맥스

### 핵심 개념

여러 개의 점수를 **확률**로 변환합니다. 모든 확률의 합은 1입니다.

### 수식

```
softmax(x_i) = e^(x_i) / Σ(e^(x_j))

여기서:
- e = 오일러 상수 (약 2.718)
- x_i = i번째 입력
- Σ(e^(x_j)) = 모든 항의 합
```

### 왜 지수(e)를 사용하는가?

```
일반 정규화: 간단하지만 음수를 잘 다루지 못함
지수 정규화: 큰 값과 작은 값의 차이를 강조

예:
입력: [1, 2, 3]

일반 정규화: [1/6, 2/6, 3/6] = [0.17, 0.33, 0.50]
소프트맥스:  [0.09, 0.24, 0.67]  ← 3이 훨씬 더 큼!
```

### 단계별 예시

```
입력: [1, 2, 3]

단계 1: 각 항의 지수 계산
  e^1 ≈ 2.718
  e^2 ≈ 7.389
  e^3 ≈ 20.086

단계 2: 합계
  합계 ≈ 2.718 + 7.389 + 20.086 ≈ 30.193

단계 3: 각 항을 합계로 나누기
  소프트맥스(1) = 2.718 / 30.193 ≈ 0.090 (9%)
  소프트맥스(2) = 7.389 / 30.193 ≈ 0.244 (24%)
  소프트맥스(3) = 20.086 / 30.193 ≈ 0.665 (67%)

단계 4: 검증 (합이 1인지 확인)
  0.090 + 0.244 + 0.665 ≈ 1.0 ✓
```

### 소프트맥스의 특징

```
입력 [1, 2, 3]
    ↓ (소프트맥스)
확률 [0.09, 0.24, 0.67]

가장 큰 값(3)이 가장 높은 확률(0.67)을 가짐
합계는 항상 1.0
모든 값이 양수 (0~1 사이)
```

### LLM에서의 실제 사용: 다음 단어 예측

LLM이 다음 단어를 선택할 때:

```
문맥: "오늘은 날씨가"

모델이 계산한 후보 단어들의 점수:
- "좋다" : 2.5
- "나쁘다" : 1.2
- "흐리다" : 0.9
- "맑다" : 2.3
- ... (수천 개)

↓ (소프트맥스 적용)

확률로 변환:
- "좋다" : 0.38 (38%)
- "나쁘다" : 0.12 (12%)
- "흐리다" : 0.07 (7%)
- "맑다" : 0.35 (35%)
- ...

↓ (선택 방법)

방법 1 (Greedy): 확률이 가장 높은 "좋다" 선택
방법 2 (Sampling): 확률에 따라 랜덤 선택
           → "좋다"가 38% 확률로 선택됨
           → "맑다"가 35% 확률로 선택됨
```

### 코드 예시

```python
import numpy as np

# 후보 단어의 점수
scores = np.array([2.5, 1.2, 0.9, 2.3])

# 소프트맥스 계산 - 방법 1: 직접 계산
exp_scores = np.exp(scores)
softmax = exp_scores / np.sum(exp_scores)
print(f"확률: {softmax}")  # [0.38 0.12 0.07 0.35]

# 소프트맥스 계산 - 방법 2: 안정적인 버전
# (큰 숫자를 다룰 때 수치 오류 방지)
scores_shifted = scores - np.max(scores)
exp_scores = np.exp(scores_shifted)
softmax_stable = exp_scores / np.sum(exp_scores)
print(f"안정적 확률: {softmax_stable}")  # 같은 결과

# 선택
words = ["좋다", "나쁘다", "흐리다", "맑다"]
chosen_word = np.random.choice(words, p=softmax)
print(f"선택된 단어: {chosen_word}")
```

### 온도(Temperature) 개념

소프트맥스를 수정해 확률 분포를 조정:

```
softmax(x / T)

T = 0.5 (낮은 온도)
↓
확률이 더 극단적 ([0.95, 0.02, 0.02, 0.01])
선택이 더 확실함 (창의성 낮음)
"좋다"가 거의 항상 선택됨

T = 1.0 (표준, 기본값)
↓
정상적인 확률 ([0.38, 0.12, 0.07, 0.35])

T = 2.0 (높은 온도)
↓
확률이 더 균등함 ([0.28, 0.24, 0.21, 0.27])
모든 단어가 더 균등하게 선택됨 (창의성 높음)
```

### 기억할 점

- 소프트맥스 = 점수를 확률로 변환
- 모든 확률의 합 = 1.0
- LLM의 다음 단어 선택 메커니즘
- 온도로 창의성 조정 가능

---

## 6. 체인 룰과 미분

### 핵심 개념

복합 함수의 미분을 계산하는 규칙입니다. LLM 학습(backpropagation)의 수학적 기초입니다.

### 기본 미분 복습

```
f(x) = x²의 미분:
f'(x) = 2x

의미: x가 1 증가할 때, f(x)는 2x만큼 증가한다
```

### 체인 룰 수식

```
y = f(g(x))일 때,

dy/dx = f'(g(x)) × g'(x)

또는 간단히:
(합성 함수의 미분) = (바깥 함수 미분) × (안쪽 함수 미분)
```

### 단계별 예시

```
문제: y = (x² + 1)³의 미분 구하기

단계 1: 함수를 두 부분으로 분해
  바깥 함수: f(u) = u³       (u = x² + 1)
  안쪽 함수: g(x) = x² + 1

단계 2: 각각 미분
  f'(u) = 3u² = 3(x² + 1)²
  g'(x) = 2x

단계 3: 곱하기 (체인 룰)
  dy/dx = 3(x² + 1)² × 2x = 6x(x² + 1)²

검증 (x = 1일 때):
  원함수: y = (1 + 1)³ = 8
  x을 1.01로 변경: y ≈ (1.0201 + 1)³ ≈ 8.122
  변화량: 8.122 - 8 = 0.122
  
  미분값: 6×1×(1+1)² = 6×4 = 24
  예상 변화량: 24 × 0.01 = 0.24 (근사 일치! ✓)
```

### 여러 단계의 체인 룰

```
y = f(g(h(x)))일 때,

dy/dx = f'(g(h(x))) × g'(h(x)) × h'(x)

(바깥쪽부터 계속 곱함)
```

### 예시: 3단계

```
문제: y = sin(x²)³의 미분

분해:
- 바깥: f(u) = u³           → f'(u) = 3u² = 3[sin(x²)]²
- 중간: g(v) = sin(v)       → g'(v) = cos(v) = cos(x²)
- 안쪽: h(x) = x²           → h'(x) = 2x

체인 룰:
dy/dx = 3[sin(x²)]² × cos(x²) × 2x = 6x·cos(x²)·[sin(x²)]²
```

### LLM에서의 역전파(Backpropagation)

LLM은 수십억 개의 파라미터를 가집니다. 학습 과정:

```
입력 → 계층1 → 계층2 → 계층3 → ... → 계층N → 출력

목표: 각 파라미터가 최종 손실(loss)에 얼마나 영향을 미치는가?

이를 계산하려면 체인 룰을 **거꾸로** 적용 (역전파)

∂Loss/∂param1 = ∂Loss/∂output × ∂output/∂층N × ... × ∂계층1/∂param1
```

### 간단한 신경망 예시

```python
# 간단한 예: y = (2x + 1)²

# 순전파 (forward pass)
x = 3
z = 2*x + 1        # z = 7
y = z**2           # y = 49

# 역전파 (backward pass, 체인 룰 사용)
# 손실이 y이면, y를 x에 대해 미분하고 싶음

# 단계 1: y의 미분값 (끝에서부터)
dy_dy = 1

# 단계 2: y에서 z로 (체인 룰 첫 단계)
dy_dz = dy_dy × d(z²)/dz = 1 × 2z = 2 × 7 = 14

# 단계 3: z에서 x로 (체인 룰 두 번째 단계)
dy_dx = dy_dz × d(2x+1)/dx = 14 × 2 = 28

# 검증
# y = (2x + 1)² 를 직접 미분하면:
# dy/dx = 2(2x + 1) × 2 = 4(2x + 1)
# x=3일 때: 4(6 + 1) = 28 ✓
```

### 기억할 점

- 체인 룰 = (바깥 미분) × (안쪽 미분)
- LLM 학습의 수학적 기초
- 역전파 = 체인 룰을 거꾸로 적용

---

## 7. 조건부 확률

### 핵심 개념

특정 조건이 주어졌을 때 어떤 일이 일어날 확률입니다.

### 수식

```
P(A|B) = P(A ∩ B) / P(B)

의미:
"B가 일어났을 때 A가 일어날 확률"

여기서:
- P(A|B) = 조건부 확률
- P(A ∩ B) = A와 B가 동시에 일어날 확률
- P(B) = B가 일어날 확률
```

### 실생활 예시

```
문제: 비가 오는 날씨에 우산을 가져올 확률?

P(우산 | 비) = P(우산 ∩ 비) / P(비)

만약:
- 100일 중 흐린 날: 40일
- 그 중 실제로 비가 온 날: 25일
- 비가 올 때 우산을 가져간 날: 24일

P(우산 ∩ 비) = 24/100 = 0.24
P(비) = 25/100 = 0.25

P(우산 | 비) = 0.24 / 0.25 = 0.96 (96%)

→ 비가 올 때 96%의 확률로 우산을 가져감
```

### 한국어 예시

```
문제: "안녕하세요"라는 인사 다음에 "반갑습니다"가 올 확률?

P("반갑습니다" | "안녕하세요") 
  = P("안녕하세요" 다음에 "반갑습니다") / P("안녕하세요"가 나타남)

텍스트 분석:
- "안녕하세요"가 나타난 횟수: 1000번
- 그 다음에 "반갑습니다"가 나타난 횟수: 850번

P("반갑습니다" | "안녕하세요") = 850 / 1000 = 0.85 (85%)

→ 자연스러운 한국어이므로 높은 확률 ✓
```

### LLM의 본질: 조건부 확률

LLM은 **조건부 확률의 연쇄**입니다:

```
사용자: "좋은 아침입니다"

LLM이 다음 단어를 예측:
1단계: P(word₁ | "좋은 아침입니다")
       → 가장 가능성 있는 단어: "오늘은"

2단계: P(word₂ | "좋은 아침입니다" + "오늘은")
       → 다음 단어: "날씨가"

3단계: P(word₃ | "좋은 아침입니다" + "오늘은" + "날씨가")
       → 다음 단어: "좋습니다"

결과: "좋은 아침입니다 오늘은 날씨가 좋습니다"
```

### 체인 룰(확률 버전)

```
P(A, B, C) = P(A) × P(B|A) × P(C|A,B)

예: 문장 생성
P("좋은 아침 오늘") 
  = P("좋은")
    × P("아침" | "좋은")
    × P("오늘" | "좋은 아침")
```

### 베이즈 정리

조건부 확률을 역으로 계산:

```
P(A|B) = P(B|A) × P(A) / P(B)

예: 검사의 정확도
질병이 있는데 양성 판정: P(양성|질병) = 0.95
질병이 없는데 양성 판정: P(양성|건강) = 0.05

만약 당신이 양성 판정을 받았을 때, 
실제로 질병이 있을 확률은?

P(질병|양성) = P(양성|질병) × P(질병) / P(양성)
```

### 코드 예시

```python
# 조건부 확률 계산

# 데이터: 날씨에 따른 외출 여부
data = [
    ("맑음", "외출"),
    ("맑음", "외출"),
    ("흐림", "외출"),
    ("흐림", "실내"),
    ("흐림", "실내"),
]

# P(외출 | 맑음) 계산
sunny_days = sum(1 for weather, action in data if weather == "맑음")
sunny_and_outside = sum(1 for weather, action in data 
                        if weather == "맑음" and action == "외출")

p_outside_given_sunny = sunny_and_outside / sunny_days
print(f"맑을 때 외출 확률: {p_outside_given_sunny}")  # 1.0 (100%)

# P(외출 | 흐림) 계산
cloudy_days = sum(1 for weather, action in data if weather == "흐림")
cloudy_and_outside = sum(1 for weather, action in data 
                         if weather == "흐림" and action == "외출")

p_outside_given_cloudy = cloudy_and_outside / cloudy_days
print(f"흐릴 때 외출 확률: {p_outside_given_cloudy}")  # 0.33 (33%)
```

### 기억할 점

- 조건부 확률 = 조건이 주어졌을 때의 확률
- LLM = 이전 단어들에 따른 다음 단어의 조건부 확률
- 체인 룰로 복합 확률 계산 가능

---

## 8. 함께 이해하기: 실제 LLM 흐름

이제 배운 모든 개념을 연결해 실제 LLM이 어떻게 작동하는지 봅시다.

### 시나리오: 간단한 답변 생성

```
사용자 질문: "내일 날씨는 어떻게 되나요?"

LLM의 내부 처리:
```

### 1단계: 텍스트를 벡터로 (임베딩)

```
"내일"      → [0.1, 0.2, -0.3, 0.5]
"날씨"      → [0.12, 0.18, -0.25, 0.55]
"어떻게"    → [0.05, 0.1, 0.2, -0.1]

각 단어가 768차원 벡터 (실제로는 더 큼)
```

### 2단계: 어텐션 메커니즘 (닷 프로덕트)

```
"내일"이 "날씨"와 얼마나 관련이 있는가?

닷 프로덕트 = 0.1×0.12 + 0.2×0.18 + (-0.3)×(-0.25) + 0.5×0.55
            = 0.012 + 0.036 + 0.075 + 0.275
            = 0.398

큰 값 → 관련이 있다는 신호
```

### 3단계: 정규화와 코사인 유사도

```
실제로는 모든 벡터가 정규화되어 있고,
닷 프로덕트 = 코사인 유사도

"내일"과 "날씨": 0.8 (매우 유사)
"내일"과 "어떻게": 0.3 (약간 관련)
```

### 4단계: 소프트맥스로 어텐션 가중치 생성

```
어텐션 점수: [0.8, 0.3, ...]

소프트맥스 적용:
e^0.8 ≈ 2.23
e^0.3 ≈ 1.35
합계 ≈ 3.58

정규화:
- "날씨": 2.23 / 3.58 ≈ 0.62 (62%)
- "어떻게": 1.35 / 3.58 ≈ 0.38 (38%)

→ 답변 생성할 때 "날씨"에 더 집중
```

### 5단계: 조건부 확률로 다음 단어 예측

```
맥락: "내일 날씨는"

후보 단어의 점수 (조건부 확률 기반):
- "좋다": 2.5
- "나쁘다": 1.2
- "흐리다": 0.9

소프트맥스:
- "좋다": 0.38 (38%)
- "나쁘다": 0.12 (12%)
- "흐리다": 0.07 (7%)

선택 (샘플링): "좋다" 선택될 확률 38%
```

### 6단계: 역전파로 모델 학습 (체인 룰)

```
실제 답변: "좋습니다"
모델 출력: "나쁩니다"

손실: 크다

역전파 (체인 룰 사용):
손실 → 출력층 → 어텐션층 → 임베딩층

각 파라미터가 얼마나 조정되어야 하는가?
체인 룰로 계산:

∂loss/∂param = ∂loss/∂output × ∂output/∂hidden × ... × ∂hidden/∂param
```

### 실제 코드 흐름 (개념적)

```python
# 1. 텍스트 토큰화 및 임베딩
text = "내일 날씨는"
tokens = tokenizer(text)
embeddings = embedding_model(tokens)

# 2. 어텐션 계산 (닷 프로덕트)
query = embeddings[0]  # "내일"
key = embeddings[1]    # "날씨는"
attention_score = np.dot(query, key)

# 3. 소프트맥스로 정규화
attention_scores = [attention_score, ...]
attention_weights = softmax(attention_scores)  # [0.62, 0.38]

# 4. 가중치 합산
context = sum(weight * embedding for weight, embedding 
              in zip(attention_weights, embeddings))

# 5. 다음 단어 점수
next_word_logits = model.forward(context)  # [2.5, 1.2, 0.9, ...]

# 6. 조건부 확률 (소프트맥스)
probabilities = softmax(next_word_logits)  # [0.38, 0.12, 0.07, ...]

# 7. 다음 단어 선택
next_word = np.random.choice(vocabulary, p=probabilities)
# 또는 greedy: next_word = vocabulary[np.argmax(probabilities)]

# 8. 학습 시: 역전파 (체인 룰)
loss = calculate_loss(next_word, expected_word)
loss.backward()  # 자동 미분 (체인 룰 내부)
optimizer.step()  # 파라미터 업데이트
```

### 함께 보는 개념 지도

```
텍스트 입력
    ↓
[벡터화] - 임베딩으로 의미 표현
    ↓
[어텐션] - 닷 프로덕트로 관련 단어 찾기
    ↓
[정규화] - 벡터 길이 표준화
    ↓
[유사도] - 코사인 유사도로 비교
    ↓
[소프트맥스] - 점수를 확률로
    ↓
[확률 분포] - 조건부 확률 계산
    ↓
[다음 단어] - 확률에 따라 선택
    ↓
[손실 계산] - 실제 답변과 비교
    ↓
[역전파] - 체인 룰로 미분
    ↓
[파라미터 업데이트] - 더 나은 답변으로
```

### 당신의 앱에서 활용하기

RAG 시스템 구현 시 이 흐름을 단순화:

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# 1. 모델 로드
model = SentenceTransformer('all-MiniLM-L6-v2')

# 2. 문서 임베딩 (미리)
documents = [
    "오늘 날씨가 좋습니다",
    "내일은 비가 올 것 같습니다",
    "모레 기온이 내려갈 예정입니다"
]
doc_embeddings = [model.encode(doc) for doc in documents]

# 3. 사용자 질문
query = "날씨 정보를 알려주세요"
query_embedding = model.encode(query)

# 4. 코사인 유사도 계산 (정규화된 벡터이므로 닷 프로덕트 = 유사도)
similarities = cosine_similarity([query_embedding], doc_embeddings)[0]

# 5. 소프트맥스로 확률로 변환 (선택사항)
from scipy.special import softmax
probabilities = softmax(similarities * 10)  # 온도=0.1

# 6. 관련 문서 선택
top_indices = np.argsort(similarities)[-2:]  # 상위 2개
relevant_docs = [documents[i] for i in top_indices]

print(f"유사도: {similarities}")
print(f"확률: {probabilities}")
print(f"관련 문서: {relevant_docs}")
```

---

## 📚 정리: 오늘의 핵심

| 개념 | 목적 | 수식 | LLM에서 |
|------|------|------|---------|
| **임베딩** | 텍스트를 숫자로 | 모델 학습 | 모든 기초 |
| **닷 프로덕트** | 두 벡터 관계 | A·B = Σ(aᵢbᵢ) | 어텐션 계산 |
| **정규화** | 크기를 1로 | v/\|v\| | 공정한 비교 |
| **코사인 유사도** | 방향 기반 유사도 | (A·B)/(ㅣAㅣㅣBㅣ) | RAG 검색 |
| **소프트맥스** | 점수→확률 | e^(xᵢ)/Σe^(xⱼ) | 다음 단어 선택 |
| **체인 룰** | 합성함수 미분 | (f∘g)' = f'·g' | 역전파 학습 |
| **조건부 확률** | 조건부 확률 | P(A\|B) = P(A∩B)/P(B) | LLM 본질 |

---

## 🎯 다음 단계

1. **복습**: 각 섹션을 한 번 더 읽기
2. **실습**: 제공된 코드를 실행해보기
3. **심화**: 
   - 행렬 연산 (행렬의 곱셈)
   - 편미분과 그래디언트
   - 경사 하강법(Gradient Descent)
   - Attention 메커니즘 상세

---

**축하합니다!** 당신은 이제 LLM의 수학적 기초를 이해했습니다. 이 지식으로 RAG, fine-tuning, 프롬프트 엔지니어링을 더 깊이 있게 접근할 수 있습니다. 🚀
