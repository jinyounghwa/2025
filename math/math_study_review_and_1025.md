# 📚 수학 공부: 복습과 오늘의 학습

날짜: 2025년 10월 25일  
학습 시간: 2-3시간 (여유 있게)  
난이도: ⭐ 낮음

---

## 📖 복습 영역 (지난 학습 내용)

### 1️⃣ 확률과 통계 기초
당신이 이미 배운 내용:
- **확률(Probability)**: 어떤 일이 일어날 가능성을 0~1 사이의 숫자로 표현
- **조건부 확률**: 한 사건이 이미 일어났을 때, 다른 사건이 일어날 확률
- **베이즈 정리**: P(A|B) = P(B|A) × P(A) / P(B) 형태로 사건 간의 관계를 계산

**간단한 복습 예제:**
```
동전을 던질 때:
- 앞이 나올 확률: 1/2 = 0.5
- 뒤가 나올 확률: 1/2 = 0.5

주사위를 던질 때:
- 3이 나올 확률: 1/6 ≈ 0.167
- 3 이상이 나올 확률: 4/6 = 2/3 ≈ 0.667
```

### 2️⃣ 로그와 지수
당신이 이미 배운 내용:
- **로그(Logarithm)**: 지수의 역함수로, "몇 번 곱해야 하는가?"라는 의미
- **로그의 밑**: 로그₂, 로그e (자연로그, ln), 로그₁₀ 등 다양한 형태
- **밑 변환 공식**: logₐ(b) = log(b) / log(a)

**간단한 복습 예제:**
```
2를 몇 번 곱해야 8이 될까?
2 × 2 × 2 = 8
따라서 log₂(8) = 3

왜 배우나?
→ 정보 크기 측정: 하나의 데이터가 담는 정보량을 비트(bit)로 측정할 때 사용
→ 복잡도 분석: 알고리즘이 얼마나 빠른지/느린지 판단
```

### 3️⃣ 순열(Permutation)과 조합(Combination)
당신이 이미 배운 내용:
- **조합**: "순서가 상관없을 때" 선택하는 가짓수 → C(n,r) = n! / (r! × (n-r)!)
- **순열**: "순서가 중요할 때" 배열하는 가짓수 → P(n,r) = n! / (n-r)!
- **차이**: {A,B} 와 {B,A} 가 같으면 조합, 다르면 순열

**간단한 복습 예제:**
```
5명 중 3명을 뽑을 때:
- 누가 뽑혔는지만 관심 → 조합: C(5,3) = 10가지
- 순서까지 중요 (1등, 2등, 3등) → 순열: P(5,3) = 60가지

왜 배우나?
→ 토크나이저 개발: 텍스트 문자들을 어떤 순서로 병합할지 선택할 때 사용
→ BPE 알고리즘에서 "어떤 쌍을 먼저 병합할 것인가?"를 결정할 때 필요
```

### 4️⃣ 엔트로피(Entropy)
당신이 이미 배운 내용:
- **정의**: 어떤 확률 분포가 얼마나 "혼란스러운지"를 측정하는 지표
- **공식**: H(X) = -Σ p(x) × log₂(p(x))  
- **의미**: 엔트로피가 높으면 불확실성이 높고, 낮으면 확실성이 높음

**간단한 복습 예제:**
```
완벽한 공정한 동전 (앞 50%, 뒤 50%):
H = -(0.5 × log₂(0.5)) - (0.5 × log₂(0.5)) = 1 bit
→ 매우 불확실함 (둘 다 나올 가능성 50-50)

편향된 동전 (앞 99%, 뒤 1%):
H = -(0.99 × log₂(0.99)) - (0.01 × log₂(0.01)) ≈ 0.08 bit
→ 거의 확실함 (앞이 거의 확실히 나올 것 같음)
```

### 5️⃣ 디시젼 트리(Decision Tree)
당신이 이미 배운 내용:
- **구조**: 루트 노드 → 내부 노드들 → 리프 노드(결과)로 이루어진 나무 구조
- **작동 원리**: "예/아니오" 질문을 반복해서 최종 답을 도출
- **정보 이득**: 엔트로피가 가장 많이 감소하는 방향으로 분할 선택

**간단한 복습 예제:**
```
대출 승인 여부 판단:

              나이 >= 25세?
             /              \
           Yes              No
           /                 \
    연봉 >= 5000만?         거절
      /          \
    Yes          No
     /            \
  승인          거절
```

---

## 🎯 오늘 배울 내용 (난이도 낮음)

### 📌 오늘의 주제: 정보 이득(Information Gain)과 엔트로피의 실전 활용

디시젼 트리에서 가장 중요한 개념인 **"정보 이득"**을 깊이 있게 이해하는 날입니다.  
이것은 당신의 토크나이저 개발과도 직접 연결됩니다!

---

## 📚 상세 학습 내용

### Part 1: 정보 이득(Information Gain)의 개념

#### 1.1 왜 정보 이득을 배워야 할까?

**상황을 생각해봅시다:**

디시젼 트리를 만들 때, 다음과 같은 선택을 해야 합니다:

```
첫 번째 분할 선택지:
① "나이"로 분할할까?
② "연봉"으로 분할할까?  
③ "신용점수"로 분할할까?

어떤 기준으로 선택할까?
```

**답:** 데이터를 가장 "깔끔하게 분리해주는" 기준을 선택해야 합니다!  
이것을 정량적으로 측정하는 것이 **정보 이득**입니다.

#### 1.2 정보 이득의 공식 (쉬운 버전)

```
정보 이득 = 분할 전 혼란도 - 분할 후 평균 혼란도
         = 분할 전 엔트로피 - (왼쪽 데이터의 가중 엔트로피 + 오른쪽 데이터의 가중 엔트로피)

기호로 표현하면:
IG = H(부모) - [|왼쪽|/|전체| × H(왼쪽) + |오른쪽|/|전체| × H(오른쪽)]
```

**직관적으로 이해하기:**

```
정보 이득이 크다는 것
= 분할 전에는 섞여 있었는데, 분할 후에는 깔끔해졌다는 뜻
= 좋은 분할이다!

정보 이득이 작다는 것  
= 분할해도 여전히 섞여 있다는 뜻
= 나쁜 분할이다!
```

#### 1.3 구체적인 예제로 이해하기

**상황: 30명의 학생을 합격/불합격으로 분류**

```
전체 학생: 30명
- 합격: 20명 (67%)
- 불합격: 10명 (33%)

분할 전 엔트로피:
H(전체) = -(0.67 × log₂(0.67)) - (0.33 × log₂(0.33))
        = -(0.67 × (-0.58)) - (0.33 × (-1.60))
        = 0.39 + 0.53
        = 0.92 bit

이제 "공부시간"으로 분할한다고 합시다:
기준: 공부시간 >= 10시간?

왼쪽 (공부시간 >= 10시간): 15명
- 합격: 14명 (93%)
- 불합격: 1명 (7%)
H(왼쪽) = -(0.93 × log₂(0.93)) - (0.07 × log₂(0.07))
        ≈ 0.38 bit

오른쪽 (공부시간 < 10시간): 15명
- 합격: 6명 (40%)
- 불합격: 9명 (60%)
H(오른쪽) = -(0.40 × log₂(0.40)) - (0.60 × log₂(0.60))
          ≈ 0.97 bit

가중 평균:
가중 엔트로피 = (15/30 × 0.38) + (15/30 × 0.97)
             = 0.19 + 0.49
             = 0.68 bit

정보 이득:
IG = 0.92 - 0.68 = 0.24 bit
```

**해석:**
- 정보 이득이 0.24 bit다 = 공부시간으로 분할하면 혼란도가 24% 감소한다는 뜻
- 이것은 좋은 분할이다!

---

### Part 2: 실전 계산 연습

#### 2.1 직접 계산해보기 (쉬운 버전)

**예제 1: 동전 분류**

```
상황: 동전 30개를 "진짜/가짜"로 분류

전체: 30개
- 진짜: 24개 (80%)
- 가짜: 6개 (20%)

"동전의 무게"로 분할한다면:
- 무거운 쪽: 15개 (진짜 12개, 가짜 3개)
- 가벼운 쪽: 15개 (진짜 12개, 가짜 3개)

이 경우 정보 이득은?
→ 거의 변화 없음 (정보 이득 ≈ 0)
→ 나쁜 분할이다!

"동전의 모양"으로 분할한다면:
- 원형: 24개 (진짜 24개, 가짜 0개) ← 완벽!
- 찌그러진 형: 6개 (진짜 0개, 가짜 6개) ← 완벽!

이 경우 정보 이득은?
→ 매우 크다!
→ 좋은 분할이다!
```

#### 2.2 정보 이득으로 최선의 분할 선택하기

```
30개의 데이터를 분류할 때 3가지 선택지가 있습니다:

분할 1 (나이): 정보 이득 = 0.31 bit
분할 2 (연봉): 정보 이득 = 0.48 bit ← 최고!
분할 3 (신용점수): 정보 이득 = 0.22 bit

→ "연봉"으로 먼저 분할하는 것이 최선이다!
```

---

### Part 3: 엔트로피 & 정보 이득과 당신의 프로젝트

#### 3.1 토크나이저(BPE) 개발과의 연결

**BPE(Byte Pair Encoding)란?**
- 텍스트를 더 효율적으로 표현하기 위해 자주 나오는 글자 쌍(pair)을 합치는 알고리즘
- 예: "th", "er", "ing" 같은 자주 나오는 쌍들을 하나의 토큰으로 만들기

**정보 이득과의 연결:**
```
질문: 다음 중 어떤 쌍을 먼저 합칠까?
- "th": 문서에서 512번 나타남
- "qu": 문서에서 8번 나타남  
- "er": 문서에서 301번 나타남

정보 이득 개념:
→ 가장 자주 나오는 쌍을 먼저 합치면
→ 텍스트를 "가장 효율적으로" 압축할 수 있다!
→ 이것이 정보 이득이다!
```

#### 3.2 디시젼 트리와 당신의 앱

**SRPG TO-DO 앱에서의 활용:**
```
사용자를 "활동적/비활동적"으로 분류할 때:

첫 번째 분할 기준을 결정할 때:
① "일일 로그인 여부"로 분할?
② "TO-DO 완료 수"로 분할?
③ "게임 플레이 시간"으로 분할?

정보 이득을 계산해서 가장 구분이 잘 되는 기준을 선택!
```

---

## ✍️ 오늘의 실습 과제 (천천히 진행하세요)

### 실습 1: 엔트로피 계산 (10분)

```
문제: 주머니에 공이 10개 있습니다.
- 빨간 공: 8개
- 파란 공: 2개

공을 무작위로 꺼낼 때의 엔트로피는?

풀이:
확률:
- 빨간 공: p₁ = 8/10 = 0.8
- 파란 공: p₂ = 2/10 = 0.2

엔트로피:
H = -(0.8 × log₂(0.8)) - (0.2 × log₂(0.2))

log₂(0.8) ≈ -0.32 (음수 주의!)
log₂(0.2) ≈ -2.32

H = -(0.8 × (-0.32)) - (0.2 × (-2.32))
  = 0.256 + 0.464
  = 0.72 bit
```

### 실습 2: 정보 이득 비교 (15분)

```
문제: 학생 20명을 합격/불합격으로 분류

전체:
- 합격: 15명 (75%)
- 불합격: 5명 (25%)

H(전체) = -(0.75 × log₂(0.75)) - (0.25 × log₂(0.25))
        ≈ 0.81 bit

분할 선택지 1: "성적 등급"으로 분할
- 상위: 10명 (합격 9명, 불합격 1명)
- 하위: 10명 (합격 6명, 불합격 4명)

H(상위) = -(0.9 × log₂(0.9)) - (0.1 × log₂(0.1)) ≈ 0.47 bit
H(하위) = -(0.6 × log₂(0.6)) - (0.4 × log₂(0.4)) ≈ 0.97 bit

가중 평균 = (10/20 × 0.47) + (10/20 × 0.97) = 0.72 bit

정보 이득 1 = 0.81 - 0.72 = 0.09 bit

분할 선택지 2: "출석률"로 분할
- 높음: 12명 (합격 12명, 불합격 0명)
- 낮음: 8명 (합격 3명, 불합격 5명)

H(높음) = 0 (완벽하게 분리됨!)
H(낮음) = -(0.375 × log₂(0.375)) - (0.625 × log₂(0.625)) ≈ 0.95 bit

가중 평균 = (12/20 × 0) + (8/20 × 0.95) = 0.38 bit

정보 이득 2 = 0.81 - 0.38 = 0.43 bit

결론: 정보 이득 2 > 정보 이득 1
→ "출석률"로 분할하는 것이 "성적 등급"으로 분할하는 것보다 훨씬 좋다!
```

---

## 🧠 핵심 정리 (이것만 기억하세요!)

### 엔트로피
```
낮은 엔트로피 = 확실성 높음 = "깔끔하게 정렬된 상태"
높은 엔트로피 = 불확실성 높음 = "섞여 있는 상태"
```

### 정보 이득
```
정보 이득 = 분할이 데이터를 얼마나 깔끔하게 정렬해주는가?

큰 정보 이득 = 좋은 분할
작은 정보 이득 = 나쁜 분할
```

### 디시젼 트리 구축 원리
```
1. 모든 가능한 분할의 정보 이득을 계산
2. 가장 큰 정보 이득을 주는 분할 선택
3. 왼쪽, 오른쪽 각각에서 반복
4. 엔트로피가 0이 될 때까지 (또는 충분히 낮아질 때까지) 계속
```

---

## 💭 생각해볼 점

1. **실생활 연결**
   - 우리가 매일 하는 선택들도 "정보 이득"을 최대화하고 있을까?
   - 예: 병원 진단에서 의사가 어떤 검사를 먼저 하는 순서도 정보 이득이 높은 것부터!

2. **당신의 프로젝트**
   - 토크나이저 개발: 정보 이득으로 가장 자주 나오는 쌍부터 병합
   - SRPG 앱: 사용자 행동 분석 시 디시젼 트리 활용 가능

3. **다음 시간 학습**
   - 상호정보(Mutual Information): 정보 이득의 더 깊은 개념
   - 엔트로피의 다양한 활용: 압축, 암호화, 데이터 분석

---

## 📌 마무리 조언

오늘은 **"정보 이득"이라는 개념을 충분히 음미하면서 배우는 시간**입니다.

✅ 서두르지 마세요 - 실습 1, 2를 천천히 직접 계산해보세요  
✅ 엔트로피와 정보 이득의 관계를 여러 각도에서 생각해보세요  
✅ 당신의 프로젝트와 어떻게 연결되는지 떠올려보세요  

모르는 부분이 있으면 언제든 물어보세요. 같이 천천히 풀어가겠습니다! 💪

---

**다음 복습일 예정:**
- 엔트로피의 성질 (최대 엔트로피, 최소 엔트로피)
- 상호정보(Mutual Information, MI)
- MI 기반 토크나이저 (WordPiece, SentencePiece)

